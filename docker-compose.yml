version: "3.8"

services:
  ml-backend:
    container_name: ml-backend
    image: humansignal/ml-backend:v0
    build:
      context: .
      args:
        TEST_ENV: ${TEST_ENV}
        USE_CUDA: ${USE_CUDA:-true}
        TORCH_CUDA_INDEX_URL: ${TORCH_CUDA_INDEX_URL:-https://download.pytorch.org/whl/cu121}
    environment:
      # specify these parameters if you want to use basic auth for the model server
      - BASIC_AUTH_USER=
      - BASIC_AUTH_PASS=
      # set the log level for the model server
      - LOG_LEVEL=DEBUG
      # any other parameters that you want to pass to the model server
      - ANY=PARAMETER
      # specify the number of workers and threads for the model server
      - WORKERS=1
      - THREADS=8
      # specify the model directory (likely you don't need to change this)
      - MODEL_DIR=/data/models

      # Specify the Label Studio URL and API key to access
      # uploaded, local storage and cloud storage files.
      # Do not use 'localhost' as it does not work within Docker containers.
      # Use prefix 'http://' or 'https://' for the URL always.
      # Determine the actual IP using 'ifconfig' (Linux/Mac) or 'ipconfig' (Windows).
      - LABEL_STUDIO_URL=http://116.103.227.252:7861
      - LABEL_STUDIO_API_KEY=ebd4ea0a8034381f1de32b9ca520e034723145bd
      # Enable NVIDIA GPU inside the container
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "9091:9090"
    volumes:
      - "./data/server:/data"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # For Docker Compose v2 CLI, you can alternatively run:
    #   docker compose up --build --gpus all
